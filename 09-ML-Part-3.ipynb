{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask-ML Part 3\n",
    "\n",
    "Here we'll look at additional scenarios including\n",
    "* Big data, low parallelism (using Dask just to support out-of-core sklearn training)\n",
    "* Big data, big parallelism with sklearn (only supports a small number of estimators)\n",
    "* Parallel scoring (both in conjunction with parallel training, and in the scoring-only scenario)\n",
    "\n",
    "### Out-of-Core Scikit-Learn via Dask\n",
    "\n",
    "This use case applies when we have a large dataset, and we're using a scikit-learn estimator that supports incremental training (`partial_fit` method).\n",
    "\n",
    "Dask can manage the \"chunking\" of the data so that we can easily train on large datasets. But __we will only have the parallelism supported by that sklearn estimator, which is usually none__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='256MB')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "\n",
    "ddf = dask.dataframe.read_csv('data/diamonds.csv', blocksize=1e6)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ddf.price\n",
    "ddf = ddf.drop(['Unnamed: 0', 'price'], axis=1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    Categorizer(),\n",
    "    DummyEncoder()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_cat = pipe.fit_transform(ddf)\n",
    "ddf_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ddf_cat, y, test_size=0.3)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The PassiveAgressiveRegressor is designed for incremental training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "est = PassiveAggressiveRegressor(n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrap the sklearn estimator in Dask's `Incremental` meta-estimator. Note the `scoring` kwarg. It is strongly recommended to pass a scoring parameter in order to ensure that a Dask-compatible metric calculation is used during training. \n",
    "\n",
    "More info at https://ml.dask.org/incremental.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "inc = Incremental(est, scoring='neg_mean_squared_error')\n",
    "inc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "neg_mse = inc.score(X_test, y_test.to_dask_array())\n",
    "math.sqrt(-neg_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so great ... but we can run multiple batches (or, here, epochs, since the data isn't so large) and perhaps converge to something better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    inc.partial_fit(X_train, y_train)\n",
    "    print('Score:', math.sqrt(-inc.score(X_test, y_test.to_dask_array())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together...\n",
    "\n",
    "For a very small number of sklearn estimators, we get support for both incremental training (batches) and parallel fitting. In this case, we can use Dask to handle scaling that data and the training.\n",
    "\n",
    "We'll try a classification problem: \"cheap/small\" diamonds (below the 25th percentile in price) vs. the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Dask array or DF APIs to threshold the response value, or use `.apply` and provide our own simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_VALUE = 1200\n",
    "\n",
    "def threshold(p):\n",
    "    return (0 if p < THRESHOLD_VALUE else 1)\n",
    "\n",
    "y_train_cat = y_train.apply(threshold, meta=('price','int64'))\n",
    "y_test_cat = y_test.apply(threshold, meta=('price','int64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use the Dask joblib backend together with the Incremental wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import joblib\n",
    "\n",
    "sgd = SGDClassifier(n_jobs=4)\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    inc2 = Incremental(sgd, scoring='accuracy')\n",
    "    inc2.fit(X_train, y_train_cat, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc2.score(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our luck with SGD we may or may not have a great solution ... we can still run multiple epochs/batches if we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    inc2.partial_fit(X_train, y_train_cat)\n",
    "    print('Score:', inc2.score(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Incremental` wrapper also parallelizes post-fit operations like `score` and `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inc2.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.partitions[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Prediction Only\n",
    "\n",
    "In the case where we have small data and can train a sklearn model locally (or load a model trained elsewhere), we can still use Dask to parallelize certain post-fit operations like `transform`, `predict`, and `predict_proba`.\n",
    "\n",
    "Dask's `ParallelPostFit` wrapper/meta-estimator can make predictions using parallel tasks for *any* sklearn estimator because, under the hood, it's basically just doing a `map_partitions` or `map_blocks` with the relevant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train) \n",
    "#note that the X_train and y_train will get `compute`d to the local VM\n",
    "\n",
    "parallel_predicting_tree = ParallelPostFit(estimator=tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_scores = parallel_predicting_tree.predict(X_test)\n",
    "da_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_scores.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "sqrt(mean_squared_error(y_test.to_dask_array(), da_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
