{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask-ML Part 2\n",
    "\n",
    "### Parallelizing Scikit-Learn\n",
    "\n",
    "We'll take a look at \n",
    "* closer interoperation with scikit-learn\n",
    "* syntax/constructs that look or work like scikit-learn\n",
    "* using Dask for high parallelism on small/medium data tasks (where data can fit in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='512MB')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "\n",
    "ddf = dask.dataframe.read_csv('data/diamonds.csv', blocksize=1e6)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ddf.price\n",
    "ddf = ddf.drop(['Unnamed: 0', 'price'], axis=1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the core data loaded, let's execute similar categorical preprocessing to our earlier example, but using `sklearn.pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    Categorizer(),\n",
    "    DummyEncoder()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.fit` to the Dask dataframe will apply the relevant `fit`, `transform`, or `fit_transform` operations for the elements within the pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... making the pipeline ready to transform the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing Dask and scikit APIs\n",
    "\n",
    "We've just seen a pipeline composed entirely of Dask drop-ins \n",
    "\n",
    "We can also operate on Dask dataframes using Dask's APIs, and pipelines that contain a mixture of Dask and scikit APIs, provided we're careful about which ones.\n",
    "\n",
    "Let's `categorize` the data via Dask's API, then see how we can use Dask's `DummyEncoder` and scikit's unmodified `RandomForestRegressor` together with Dask's joblib backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_cat = ddf.categorize()\n",
    "ddf_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ddf_cat, y, test_size=0.3)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "* joblib Dask backend specified as a context manager\n",
    "* `n_jobs` specified for the RandomForestRegressor\n",
    "\n",
    "This approach to parallel training only works where the sklearn estimator supports multiple jobs via joblib. The relevant classes are not well documented, but are typically ones with `n_jobs` as a constructor argument.\n",
    "\n",
    "There's an open issue (around the sklearn documentation) at:\n",
    "* https://github.com/scikit-learn/scikit-learn/issues/14228\n",
    "\n",
    "And a list generated from the source code at: \n",
    "* https://gist.github.com/cmarmo/f8cd0f4c82f8fc816a106fd3510c61dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "pipe_2 = make_pipeline(\n",
    "    DummyEncoder(),\n",
    "    RandomForestRegressor(n_jobs=4)\n",
    ")\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pipe_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although the training involves parallel tasked scheduled by Dask, the pipeline itself is not parallel-aware, and so when we call `.predict` that is a local operation returning a regular ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = pipe_2.predict(X_test)\n",
    "y_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X_test were very large -- or we wanted a production system that was scoring many records in parallel -- Dask provides a wrapper to parallelize the \"post-fit\" operations such as predict and score. We'll look at that in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(y_test.compute(), y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
